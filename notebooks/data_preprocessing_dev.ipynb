{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f61e464-4ba2-495e-bd78-273d0c28749c",
   "metadata": {},
   "source": [
    "# Data Preprocessing Dev\n",
    "\n",
    "1. **Load and Preprocess Evidence Data**:\n",
    "\n",
    "- *Data Structure*: Your dataset, evidence_df, contains two columns: evidence_id and evidence_paragraph.\n",
    "- *Objective*: Use all evidence paragraphs to train a TF-IDF model. This model will be used to retrieve the most relevant evidences for a given input claim.\n",
    "\n",
    "2. **TF-IDF for Evidence Retrieval**:\n",
    "\n",
    "- *Preprocessing*: Clean and preprocess the evidence paragraphs to optimize them for TF-IDF vectorization (e.g., removing stopwords, punctuation, and normalizing text).\n",
    "- *Vectorization*: Apply TF-IDF vectorization to the preprocessed evidence paragraphs to create a matrix representing the importance of terms in each document.\n",
    "- *Similarity Calculation*: When a new claim is received, convert it into a TF-IDF vector using the same vectorizer and calculate its cosine similarity against the TF-IDF matrix to find the most relevant evidences.\n",
    "\n",
    "3. **Construct an Evidence List**:\n",
    "\n",
    "*Relevance*: Based on the similarity scores, select the top relevant evidences. This list will be used for further processing and classification.\n",
    "\n",
    "4. **Concatenate Claim and Evidences**:\n",
    "\n",
    "*Integration*: Concatenate the input claim with its corresponding top relevant evidences into a single text block (paragraph). This concatenated text serves as a comprehensive context for the claim.\n",
    "\n",
    "5. **Word2Vec Model Training and Application**:\n",
    "\n",
    "- *Model Building*: Build a Word2Vec model from scratch using PyTorch to learn word embeddings from the concatenated text of claims and their relevant evidences.\n",
    "- *Usage*: The trained Word2Vec model can be used to convert words or phrases from the claims and evidences into vectors, which can then be utilized for various tasks such as classification, clustering, or further similarity measurements.\n",
    "\n",
    "6. **Classification**:\n",
    "\n",
    "- *Approach*: Use the embeddings from the Word2Vec model along with additional features (if necessary) to classify the claim into one of four predefined categories.\n",
    "- *Model Selection*: Depending on the complexity and nature of the classification, choose an appropriate machine learning or deep learning model. This could be a simple logistic regression, a support vector machine, or a more complex neural network.\n",
    "\n",
    "**Considerations for Implementation**:\n",
    "- *Modularity*: Each step should be encapsulated within its class or function to ensure modularity and ease of maintenance.\n",
    "- *Scalability*: Design the system to handle increases in data volume efficiently, possibly by optimizing data handling and processing.\n",
    "- *Extensibility*: Allow for easy updates and modifications, such as adding new preprocessing steps, changing the classification model, or adjusting the number of top evidences retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49d6cc-043e-4415-819e-48545d8a2fdd",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Evidence Data\n",
    "\n",
    "- *Data Structure*: Your dataset, evidence_df, contains two columns: evidence_id and evidence_paragraph.\n",
    "- *Objective*: Use all evidence paragraphs to train a TF-IDF model. This model will be used to retrieve the most relevant evidences for a given input claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a1469b-da37-49cb-b087-3ecb87e8a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9716d4e-be2d-4b9f-a10e-0ceaa4d98265",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataLoader:\n",
    "    file_path: Path\n",
    "\n",
    "    def load_data(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads the data from the specified JSON file path using a Path object.\n",
    "        Attempts to read a JSON file into a pandas DataFrame.\n",
    "        Logs an error and returns None if the operation fails.\n",
    "        \n",
    "        :return: Optional[pd.DataFrame] - A pandas DataFrame if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.file_path.exists():\n",
    "                logger.warning(f\"The file {self.file_path} does not exist.\")\n",
    "                return None\n",
    "            \n",
    "            data = pd.read_json(self.file_path, orient='index')\n",
    "            logger.info(\"Data loaded successfully.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"An error occurred while loading the data from {self.file_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31628f04-01c2-43ea-92fc-33811cf9087e",
   "metadata": {},
   "source": [
    "## 2. **TF-IDF for Evidence Retrieval**:\n",
    "\n",
    "Text preprocessing is a critical step in our pipeline, aiming to transform raw text into a more analyzable and meaningful format. This step involves cleaning the text, reducing words to their base or root form, and removing irrelevant characters and words that do not contribute to the semantic meaning of the text.\n",
    "\n",
    "### Approach\n",
    "Our preprocessing workflow integrates several techniques to refine the text data:\n",
    "\n",
    "1. **Contraction Expansion**: Converts contractions (e.g., \"isn't\" to \"is not\") to their expanded form to standardize text and improve analysis accuracy.\n",
    "2. **Lowercasing**: Transforms all text to lowercase to ensure consistency and avoid duplication based on case differences.\n",
    "3. **Special Characters Removal**: Deletes non-word characters (e.g., punctuation) to focus on the textual content.\n",
    "4. **Tokenization**: Splits text into individual words or tokens, facilitating further processing like part-of-speech tagging.\n",
    "5. **Part-of-Speech Tagging**: Identifies the grammatical parts of speech of each word, which helps in lemmatization.\n",
    "6. **Lemmatization**: Reduces words to their base or dictionary form, considering the word's part-of-speech to ensure that the root word (lemma) is a valid word.\n",
    "7. **Stop Words Removal**: Eliminates commonly used words (e.g., \"the\", \"is\") that usually have little to no semantic value in the context of text analysis.\n",
    "8. **Named Entity Recognition (NER)**: Identifies and preserves named entities (e.g., \"South Australia\") as unique tokens. This is crucial for maintaining the specificity of geographical locations, organizations, and individuals in the text.\n",
    "9. **Contextual Token Support**: Enhances the representation of text by considering the context around important words or named entities. This approach helps in capturing the semantic meaning more effectively.\n",
    "\n",
    "### Implementation\n",
    "The `preprocess_text` method encapsulates the preprocessing steps, taking a string of text and an index as inputs. The index allows for logging progress at specified intervals, enhancing transparency and monitoring during processing.\n",
    "\n",
    "During the preprocessing, after tokenization and part-of-speech tagging, we perform **Named Entity Recognition (NER)** using NLTK's `ne_chunk`. Named entities are combined into single tokens (e.g., \"New York\" becomes \"New_York\"), which are then processed along with other tokens for lemmatization and stop words removal.\n",
    "\n",
    "Additionally, we incorporate **contextual token support** by examining the context around key terms and named entities. This allows our system to better understand the relevance and significance of specific phrases within the text, thereby improving the accuracy of evidence retrieval.\n",
    "\n",
    "The `preprocess` method orchestrates the preprocessing of the entire dataset. It utilizes the tqdm library to display a progress bar, providing real-time feedback on the preprocessing status.\n",
    "\n",
    "### Example\n",
    "Consider the claim: \"[South Australia] has the most expensive electricity in the world.\" During preprocessing:\n",
    "\n",
    "1. **Contraction Expansion & Lowercasing**: No contractions present; \"South Australia\" is lowercased.\n",
    "2. **Special Characters Removal**: Assumes no special characters; the text remains unchanged.\n",
    "3. **Tokenization & Part-of-Speech Tagging**: Splits the text into tokens and tags them.\n",
    "4. **Named Entity Recognition (NER)**: Identifies \"South Australia\" as a named entity and preserves it as a unique token.\n",
    "5. **Lemmatization & Stop Words Removal**: \"has\" -> \"have\", \"expensive\" remains, \"electricity\" remains, removing \"the\", \"in\", \"world\".\n",
    "\n",
    "The preprocessing results in a focused representation of the claim, highlighting the key components and preserving the named entity \"South Australia\" for precise evidence retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c2b12859-2e77-438e-ab66-8eb4b2fd01a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import contractions\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "64f6ab77-0ab8-48d1-a47c-469f960ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TfidfEvidenceRetriever:\n",
    "    evidence_path: Path\n",
    "    vectorizer: TfidfVectorizer = field(default_factory=lambda: TfidfVectorizer(ngram_range=(1, 3), max_df=0.85, min_df=2))\n",
    "    tfidf_matrix: np.ndarray = None\n",
    "    stop_words: set = field(default_factory=lambda: set(stopwords.words('english')))\n",
    "    similarity_threshold: float = 0.45\n",
    "    lemmatizer: WordNetLemmatizer = field(default_factory=WordNetLemmatizer)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.evidence_df = pd.read_json(self.evidence_path, orient='index').head(100000)\n",
    "        self.evidence_df.columns = ['paragraph']\n",
    "        self.evidence_df.reset_index(inplace=True)\n",
    "        self.preprocess()\n",
    "\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        \"\"\"Converts treebank POS tags to WordNet POS tags.\"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def enrich_text(self, word, pos, existing_tokens):\n",
    "        \"\"\"Enriches text based on POS, avoiding duplicates.\"\"\"\n",
    "        enrichments = []\n",
    "        synsets = wordnet.synsets(word, pos=pos)\n",
    "        if not synsets:\n",
    "            return enrichments\n",
    "\n",
    "        # For nouns: add at least one synonym and two hyponyms\n",
    "        if pos == wordnet.NOUN:\n",
    "            synonyms_added, hyponyms_added = 0, 0\n",
    "            for synset in synsets:\n",
    "                if synonyms_added < 1:\n",
    "                    for lemma in synset.lemmas():\n",
    "                        lemma_name = lemma.name().replace('_', ' ').lower()\n",
    "                        if lemma_name != word and lemma_name not in existing_tokens:\n",
    "                            enrichments.append(lemma_name)\n",
    "                            synonyms_added += 1\n",
    "                            break  # Break after adding one synonym\n",
    "                for hyponym in synset.hyponyms():\n",
    "                    for lemma in hyponym.lemmas():\n",
    "                        lemma_name = lemma.name().replace('_', ' ').lower()\n",
    "                        if lemma_name != word and lemma_name not in existing_tokens and hyponyms_added < 2:\n",
    "                            enrichments.append(lemma_name)\n",
    "                            hyponyms_added += 1\n",
    "                            existing_tokens.add(lemma_name)\n",
    "                        if hyponyms_added >= 2:\n",
    "                            break\n",
    "                    if hyponyms_added >= 2:\n",
    "                        break\n",
    "\n",
    "        # For verbs: add an antonym\n",
    "        if pos == wordnet.VERB:\n",
    "            for synset in synsets:\n",
    "                for lemma in synset.lemmas():\n",
    "                    if lemma.antonyms():\n",
    "                        antonym_name = lemma.antonyms()[0].name().replace('_', ' ').lower()\n",
    "                        if antonym_name not in existing_tokens:\n",
    "                            enrichments.append(antonym_name)\n",
    "                            break\n",
    "\n",
    "        return enrichments\n",
    "\n",
    "    def traverse_tree(self, tree):\n",
    "            final_tokens = []\n",
    "            for subtree in tree:\n",
    "                if type(subtree) == Tree:\n",
    "                    ne_token = \"_\".join(word for word, tag in subtree.leaves())\n",
    "                    final_tokens.append(ne_token)\n",
    "                else:\n",
    "                    final_tokens.append(subtree[0])\n",
    "            return final_tokens\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Updated preprocess text method to include text enrichment based on POS.\"\"\"\n",
    "        text = contractions.fix(text)  # Expand contractions\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        \n",
    "        ne_tree = ne_chunk(tagged_tokens)\n",
    "        \n",
    "        processed_tokens = self.traverse_tree(ne_tree)\n",
    "        \n",
    "        seen_tokens = set()\n",
    "        final_tokens = []\n",
    "        for token, tag in tagged_tokens:\n",
    "            wordnet_pos = self.get_wordnet_pos(tag)  # Convert POS tag to a WordNet POS tag.\n",
    "            if wordnet_pos:  # Only enrich if a valid WordNet POS tag is available.\n",
    "                enrichments = self.enrich_text(token, wordnet_pos, seen_tokens)\n",
    "                for enrichment in enrichments:\n",
    "                    if enrichment not in seen_tokens:\n",
    "                        final_tokens.append(enrichment)\n",
    "                        seen_tokens.add(enrichment)\n",
    "            # Add the original token if not already added.\n",
    "            token_lower = token.lower()\n",
    "            if token_lower not in seen_tokens:\n",
    "                final_tokens.append(token_lower)\n",
    "                seen_tokens.add(token_lower)\n",
    "        \n",
    "        final_text = ' '.join(final_tokens)\n",
    "        final_text = re.sub(r'[^\\w\\s]', '', final_text)\n",
    "        final_text = re.sub(r'\\s{2,}', ' ', final_text)\n",
    "        \n",
    "        return final_text.strip(' ')\n",
    "\n",
    "    def preprocess(self):\n",
    "        logger.info(\"Starting preprocessing of paragraphs.\")\n",
    "        processed_paragraphs = [self.preprocess_text(paragraph) for paragraph in tqdm(self.evidence_df['paragraph'], desc=\"Preprocessing paragraphs\")]\n",
    "        self.evidence_df['processed_paragraph'] = processed_paragraphs\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.evidence_df['processed_paragraph'])\n",
    "        logger.info(\"Preprocessing complete.\")\n",
    "\n",
    "    def find_relevant_evidences(self, query: str) -> pd.DataFrame:\n",
    "        processed_query = self.preprocess_text(query)\n",
    "        logger.info(f\"Processed claim: {processed_query}\")\n",
    "        query_tfidf = self.vectorizer.transform([processed_query])\n",
    "        cosine_similarities = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()\n",
    "\n",
    "        sorted_indices = np.argsort(cosine_similarities)[::-1]\n",
    "        if cosine_similarities[sorted_indices[0]] < self.similarity_threshold:\n",
    "            most_relevant_index = [sorted_indices[0]]\n",
    "            logger.info(\"No evidences above the threshold. Returning the most relevant evidence.\")\n",
    "            return self.evidence_df.iloc[most_relevant_index]\n",
    "\n",
    "        relevant_indices = [index for index in sorted_indices if cosine_similarities[index] >= self.similarity_threshold][:6]\n",
    "        logger.info(f\"Found {len(relevant_indices)} relevant evidences.\")\n",
    "        return self.evidence_df.iloc[relevant_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cafb30-8af0-45d8-b29b-dc459add86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 01:37:07 - INFO - Starting preprocessing of paragraphs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57add4d6b76845d88f3e05877f9cb748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing paragraphs:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming evidence_df is already loaded and contains a 'paragraph' column\n",
    "retriever = TfidfEvidenceRetriever(Path('../data/evidence.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775733d-15d0-4378-b65b-e1de9830688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(retriever.evidence_df.iloc[67732])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070237a5-7aa6-491a-9035-e35d64eeee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.preprocess_text(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4893e68-9af7-48c0-b5e7-503a321c9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"[South Australia] has the most expensive electricity in the world.\"\n",
    "retriever.similarity_threshold=0.2\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "relevant_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6e927-2631-4523-8d9a-a0b0098c8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"The actual data show high northern latitudes are warmer today than in 1940.\"\n",
    "retriever.similarity_threshold=0.1\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "relevant_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4826cbd-b892-43ef-b0f9-5b6928a8504f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e5b2d60-e44c-4237-8387-d4753c6d871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 23:02:32 - INFO - Processed claim: satellite measure infrared spectrum past 40 years observe less free energy escape infinite wavelength consort co2 .\n",
      "2024-05-12 23:02:32 - INFO - No evidences above the threshold. Returning the most relevant evidence.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>processed_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51380</th>\n",
       "      <td>evidence-51380</td>\n",
       "      <td>The Weebles 1971 -- 2011 Price Guide and Index...</td>\n",
       "      <td>weebles 1971 -- 2011 price guide index al-Qur'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index                                          paragraph  \\\n",
       "51380  evidence-51380  The Weebles 1971 -- 2011 Price Guide and Index...   \n",
       "\n",
       "                                     processed_paragraph  \n",
       "51380  weebles 1971 -- 2011 price guide index al-Qur'...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"Satellite measurements of infrared spectra over the past 40 years observe less energy escaping to space at the wavelengths associated with CO2.\"\n",
    "retriever.similarity_threshold=0.5\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "relevant_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33daef-8d9f-4357-b792-51ef31b55276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d497176-6abe-4a4c-9938-710683c9705d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b53356-c850-49df-b17f-801aa70a498e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdaa9b8-2f52-4af4-aea8-8962c86acc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tfidf_evidence_retriever.pkl', 'rb') as file:\n",
    "#    loaded_retriever = pickle.load(file)\n",
    "#logger.info(\"Retriever object loaded sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45680a5-a6ac-407e-abfc-0b6e50996885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever object saved successfully.\n"
     ]
    }
   ],
   "source": [
    "with open('tfidf_evidence_retriever.pkl', 'wb') as file:\n",
    "    pickle.dump(retriever, file)\n",
    "\n",
    "print(\"Retriever object saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e24bdb7-ffd8-4f9c-9fe7-8fb81f68d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 21:43:34 - INFO - Finding relevant evidences for the given query: 'when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.'\n",
      "2024-05-12 21:43:35 - INFO - Found 2 relevant evidences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1140012</th>\n",
       "      <td>Developing countries with the highest rate of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78654</th>\n",
       "      <td>\", as opposed to \"per cent\".</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paragraph\n",
       "1140012  Developing countries with the highest rate of ...\n",
       "78654                         \", as opposed to \"per cent\"."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.\"\n",
    "retriever.similarity_threshold=0.5\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "pd.DataFrame(retriever.evidence_df.iloc[relevant_indices]['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c65ac82-3a3a-4134-adaf-9d7d21faa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 20:51:53 - INFO - Finding relevant evidences for the given query: 'when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.'\n",
      "2024-05-12 20:51:53 - INFO - No evidences above the threshold. Returning the most relevant evidence.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>Their reported relationship appeared to accoun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              paragraph\n",
       "5606  Their reported relationship appeared to accoun..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.\"\n",
    "retriever.similarity_threshold=0.5\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "pd.DataFrame(retriever.evidence_df.iloc[relevant_indices]['paragraph'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.8.19",
   "language": "python",
   "name": "python3.8.19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
