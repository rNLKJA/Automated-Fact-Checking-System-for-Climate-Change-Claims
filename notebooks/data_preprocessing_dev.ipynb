{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f61e464-4ba2-495e-bd78-273d0c28749c",
   "metadata": {},
   "source": [
    "# Data Preprocessing Dev\n",
    "\n",
    "1. **Load and Preprocess Evidence Data**:\n",
    "\n",
    "- *Data Structure*: Your dataset, evidence_df, contains two columns: evidence_id and evidence_paragraph.\n",
    "- *Objective*: Use all evidence paragraphs to train a TF-IDF model. This model will be used to retrieve the most relevant evidences for a given input claim.\n",
    "\n",
    "2. **TF-IDF for Evidence Retrieval**:\n",
    "\n",
    "- *Preprocessing*: Clean and preprocess the evidence paragraphs to optimize them for TF-IDF vectorization (e.g., removing stopwords, punctuation, and normalizing text).\n",
    "- *Vectorization*: Apply TF-IDF vectorization to the preprocessed evidence paragraphs to create a matrix representing the importance of terms in each document.\n",
    "- *Similarity Calculation*: When a new claim is received, convert it into a TF-IDF vector using the same vectorizer and calculate its cosine similarity against the TF-IDF matrix to find the most relevant evidences.\n",
    "\n",
    "3. **Construct an Evidence List**:\n",
    "\n",
    "*Relevance*: Based on the similarity scores, select the top relevant evidences. This list will be used for further processing and classification.\n",
    "\n",
    "4. **Concatenate Claim and Evidences**:\n",
    "\n",
    "*Integration*: Concatenate the input claim with its corresponding top relevant evidences into a single text block (paragraph). This concatenated text serves as a comprehensive context for the claim.\n",
    "\n",
    "5. **Word2Vec Model Training and Application**:\n",
    "\n",
    "- *Model Building*: Build a Word2Vec model from scratch using PyTorch to learn word embeddings from the concatenated text of claims and their relevant evidences.\n",
    "- *Usage*: The trained Word2Vec model can be used to convert words or phrases from the claims and evidences into vectors, which can then be utilized for various tasks such as classification, clustering, or further similarity measurements.\n",
    "\n",
    "6. **Classification**:\n",
    "\n",
    "- *Approach*: Use the embeddings from the Word2Vec model along with additional features (if necessary) to classify the claim into one of four predefined categories.\n",
    "- *Model Selection*: Depending on the complexity and nature of the classification, choose an appropriate machine learning or deep learning model. This could be a simple logistic regression, a support vector machine, or a more complex neural network.\n",
    "\n",
    "**Considerations for Implementation**:\n",
    "- *Modularity*: Each step should be encapsulated within its class or function to ensure modularity and ease of maintenance.\n",
    "- *Scalability*: Design the system to handle increases in data volume efficiently, possibly by optimizing data handling and processing.\n",
    "- *Extensibility*: Allow for easy updates and modifications, such as adding new preprocessing steps, changing the classification model, or adjusting the number of top evidences retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49d6cc-043e-4415-819e-48545d8a2fdd",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Evidence Data\n",
    "\n",
    "- *Data Structure*: Your dataset, evidence_df, contains two columns: evidence_id and evidence_paragraph.\n",
    "- *Objective*: Use all evidence paragraphs to train a TF-IDF model. This model will be used to retrieve the most relevant evidences for a given input claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a1469b-da37-49cb-b087-3ecb87e8a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9716d4e-be2d-4b9f-a10e-0ceaa4d98265",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataLoader:\n",
    "    file_path: Path\n",
    "\n",
    "    def load_data(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads the data from the specified JSON file path using a Path object.\n",
    "        Attempts to read a JSON file into a pandas DataFrame.\n",
    "        Logs an error and returns None if the operation fails.\n",
    "        \n",
    "        :return: Optional[pd.DataFrame] - A pandas DataFrame if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.file_path.exists():\n",
    "                logger.warning(f\"The file {self.file_path} does not exist.\")\n",
    "                return None\n",
    "            \n",
    "            data = pd.read_json(self.file_path, orient='index')\n",
    "            logger.info(\"Data loaded successfully.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"An error occurred while loading the data from {self.file_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31628f04-01c2-43ea-92fc-33811cf9087e",
   "metadata": {},
   "source": [
    "## 2. **TF-IDF for Evidence Retrieval**:\n",
    "\n",
    "Text preprocessing is a critical step in our pipeline, aiming to transform raw text into a more analyzable and meaningful format. This step involves cleaning the text, reducing words to their base or root form, and removing irrelevant characters and words that do not contribute to the semantic meaning of the text.\n",
    "\n",
    "### Approach\n",
    "Our preprocessing workflow integrates several techniques to refine the text data:\n",
    "\n",
    "1. **Contraction Expansion**: Converts contractions (e.g., \"isn't\" to \"is not\") to their expanded form to standardize text and improve analysis accuracy.\n",
    "2. **Lowercasing**: Transforms all text to lowercase to ensure consistency and avoid duplication based on case differences.\n",
    "3. **Special Characters Removal**: Deletes non-word characters (e.g., punctuation) to focus on the textual content.\n",
    "4. **Tokenization**: Splits text into individual words or tokens, facilitating further processing like part-of-speech tagging.\n",
    "5. **Part-of-Speech Tagging**: Identifies the grammatical parts of speech of each word, which helps in lemmatization.\n",
    "6. **Lemmatization**: Reduces words to their base or dictionary form, considering the word's part-of-speech to ensure that the root word (lemma) is a valid word.\n",
    "7. **Stop Words Removal**: Eliminates commonly used words (e.g., \"the\", \"is\") that usually have little to no semantic value in the context of text analysis.\n",
    "8. **Named Entity Recognition (NER)**: Identifies and preserves named entities (e.g., \"South Australia\") as unique tokens. This is crucial for maintaining the specificity of geographical locations, organizations, and individuals in the text.\n",
    "9. **Contextual Token Support**: Enhances the representation of text by considering the context around important words or named entities. This approach helps in capturing the semantic meaning more effectively.\n",
    "\n",
    "### Implementation\n",
    "The `preprocess_text` method encapsulates the preprocessing steps, taking a string of text and an index as inputs. The index allows for logging progress at specified intervals, enhancing transparency and monitoring during processing.\n",
    "\n",
    "During the preprocessing, after tokenization and part-of-speech tagging, we perform **Named Entity Recognition (NER)** using NLTK's `ne_chunk`. Named entities are combined into single tokens (e.g., \"New York\" becomes \"New_York\"), which are then processed along with other tokens for lemmatization and stop words removal.\n",
    "\n",
    "Additionally, we incorporate **contextual token support** by examining the context around key terms and named entities. This allows our system to better understand the relevance and significance of specific phrases within the text, thereby improving the accuracy of evidence retrieval.\n",
    "\n",
    "The `preprocess` method orchestrates the preprocessing of the entire dataset. It utilizes the tqdm library to display a progress bar, providing real-time feedback on the preprocessing status.\n",
    "\n",
    "### Example\n",
    "Consider the claim: \"[South Australia] has the most expensive electricity in the world.\" During preprocessing:\n",
    "\n",
    "1. **Contraction Expansion & Lowercasing**: No contractions present; \"South Australia\" is lowercased.\n",
    "2. **Special Characters Removal**: Assumes no special characters; the text remains unchanged.\n",
    "3. **Tokenization & Part-of-Speech Tagging**: Splits the text into tokens and tags them.\n",
    "4. **Named Entity Recognition (NER)**: Identifies \"South Australia\" as a named entity and preserves it as a unique token.\n",
    "5. **Lemmatization & Stop Words Removal**: \"has\" -> \"have\", \"expensive\" remains, \"electricity\" remains, removing \"the\", \"in\", \"world\".\n",
    "\n",
    "The preprocessing results in a focused representation of the claim, highlighting the key components and preserving the named entity \"South Australia\" for precise evidence retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b12859-2e77-438e-ab66-8eb4b2fd01a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "import contractions\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f6ab77-0ab8-48d1-a47c-469f960ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TfidfEvidenceRetriever:\n",
    "    evidence_path: Path\n",
    "    vectorizer: TfidfVectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.85, min_df=2)\n",
    "    tfidf_matrix: np.ndarray = None\n",
    "    stop_words: set = field(default_factory=lambda: set(stopwords.words('english')))\n",
    "    similarity_threshold: float = 0.45\n",
    "    lemmatizer: WordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.evidence_df = pd.read_json(self.evidence_path, orient='index')\n",
    "        self.evidence_df.columns = ['paragraph']\n",
    "        self.evidence_df.reset_index(inplace=True)\n",
    "        if 'paragraph' not in self.evidence_df.columns:\n",
    "            logger.error(\"DataFrame must contain a 'paragraph' column.\")\n",
    "            raise ValueError(\"DataFrame must contain a 'paragraph' column.\")\n",
    "        self.preprocess()\n",
    "\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        \"\"\"Converts treebank POS tags to WordNet POS tags.\"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to noun\n",
    "\n",
    "    def preprocess_text(self, text: str, index: int) -> str:\n",
    "        text = contractions.fix(text.lower())\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        chunked_nes = ne_chunk(tagged_tokens)\n",
    "        nes = [\"_\".join(w for w, t in ne.leaves()) for ne in chunked_nes if isinstance(ne, nltk.Tree)]\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(tag)) for word, tag in tagged_tokens if word not in self.stop_words and word not in nes]\n",
    "        combined_tokens = nes + lemmatized_tokens\n",
    "        return ' '.join(combined_tokens)\n",
    "\n",
    "    def preprocess(self):\n",
    "        logger.info(\"Starting preprocessing of paragraphs.\")\n",
    "        processed_paragraphs = [self.preprocess_text(paragraph, i) for i, paragraph in tqdm(enumerate(self.evidence_df['paragraph']), total=self.evidence_df.shape[0])]\n",
    "        self.evidence_df['processed_paragraph'] = processed_paragraphs\n",
    "        logger.info(\"Vectorizing processed paragraphs.\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.evidence_df['processed_paragraph'])\n",
    "        logger.info(\"Preprocessing complete.\")\n",
    "\n",
    "    def find_relevant_evidences(self, query: str) -> list:\n",
    "        logger.info(f\"Finding relevant evidences for the given query: '{query}'\")\n",
    "        processed_query = self.preprocess_text(query, -1)  # -1 index since it's just a single query\n",
    "        query_tfidf = self.vectorizer.transform([processed_query])\n",
    "        cosine_similarities = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        if cosine_similarities.max() < self.similarity_threshold:\n",
    "            most_relevant_index = [np.argmax(cosine_similarities)]\n",
    "            logger.info(\"No evidences above the threshold. Returning the most relevant evidence.\")\n",
    "            return most_relevant_index\n",
    "        \n",
    "        relevant_indices = [index for index, similarity in enumerate(cosine_similarities) if similarity >= self.similarity_threshold]\n",
    "        logger.info(f\"Found {len(relevant_indices)} relevant evidences.\")\n",
    "        return relevant_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cafb30-8af0-45d8-b29b-dc459add86da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 20:02:23 - INFO - Starting preprocessing of paragraphs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85253dd3c0ac4497b2b24c88add7c35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208827 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming evidence_df is already loaded and contains a 'paragraph' column\n",
    "retriever = TfidfEvidenceRetriever(Path('../data/evidence.json'))\n",
    "retriever.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7e24bdb7-ffd8-4f9c-9fe7-8fb81f68d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 19:35:03 - INFO - Finding relevant evidences for the given query: when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.\n",
      "2024-05-12 19:35:04 - INFO - Found 2 relevant evidences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78654</th>\n",
       "      <td>\", as opposed to \"per cent\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140012</th>\n",
       "      <td>Developing countries with the highest rate of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paragraph\n",
       "78654                         \", as opposed to \"per cent\".\n",
       "1140012  Developing countries with the highest rate of ..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.\"\n",
    "retriever.similarity_threshold=0.5\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "pd.DataFrame(evidence_df.iloc[relevant_indices]['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b4893e68-9af7-48c0-b5e7-503a321c9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 19:45:35 - INFO - Finding relevant evidences for the given query: [South Australia] has the most expensive electricity in the world.\n",
      "2024-05-12 19:45:35 - INFO - Found 2 relevant evidences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995049</th>\n",
       "      <td>The Attorney-General of South Australia is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095235</th>\n",
       "      <td>'' For the place in Adelaide, South Australia,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paragraph\n",
       "995049   The Attorney-General of South Australia is the...\n",
       "1095235  '' For the place in Adelaide, South Australia,..."
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"[South Australia] has the most expensive electricity in the world.\"\n",
    "retriever.similarity_threshold=0.30\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "pd.DataFrame(evidence_df.iloc[relevant_indices]['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b81aca48-b500-45f7-b7b6-ae9482d1f17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is the state animal of South Australia.',\n",
       " 'It is found in South Australia and Western Australia.',\n",
       " \"The District Court of South Australia is South Australia 's principal trial court.\",\n",
       " 'Power FM (South Australia), a radio station in South Australia, Australia',\n",
       " 'The Cabinet of South Australia is the chief policy-making organ of the Government of South Australia.',\n",
       " 'Copeville is a settlement in South Australia.',\n",
       " 'Australia',\n",
       " 'The South Australia Colonisation Act 1834 (4 & 5 Will.',\n",
       " \"The Attorney-General of South Australia is the member of the Government of South Australia responsible for South Australia 's system of law and justice.\",\n",
       " \"'' For the place in Adelaide, South Australia, see Glynde, South Australia.\",\n",
       " 'Port Vincent, South Australia, Australia',\n",
       " 'Mitcham, South Australia, a suburb of Adelaide, South Australia']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pd.DataFrame(evidence_df.iloc[relevant_indices]['paragraph'])['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2c65ac82-3a3a-4134-adaf-9d7d21faa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 19:40:56 - INFO - Finding relevant evidences for the given query: when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.\n",
      "2024-05-12 19:40:56 - INFO - Found 2 relevant evidences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\", as opposed to \"per cent\".',\n",
       " 'Developing countries with the highest rate of women who have been cut are Somalia (with 98 per cent of women affected), Guinea (96 per cent), Djibouti (93 per cent), Egypt (91 per cent), Eritrea (89 per cent), Mali (89 per cent), Sierra Leone (88 per cent), Sudan (88 per cent), Gambia (76 per cent), Burkina Faso (76 per cent), and Ethiopia (74 per cent).']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example claim to retrieve evidence for\n",
    "claim = \"when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate.\"\n",
    "retriever.similarity_threshold=0.5\n",
    "relevant_indices = retriever.find_relevant_evidences(claim)\n",
    "list(pd.DataFrame(evidence_df.iloc[relevant_indices]['paragraph'])['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2351042d-6959-42f7-987a-6140cdd78a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[citation needed] South Australia has the highest retail price for electricity in the country.']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(evidence_df[evidence_df['index'] == 'evidence-67732']['paragraph'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.8.19",
   "language": "python",
   "name": "python3.8.19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
