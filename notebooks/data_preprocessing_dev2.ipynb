{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1087d752-ed8e-49f5-a4cd-55668cf030b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Precompile regex patterns\n",
    "punctuation_pattern = re.compile(f\"[{string.punctuation}’]\")\n",
    "specific_chars_pattern = re.compile(r'[\\'\\\"]')\n",
    "multi_space_pattern = re.compile(r'\\s{2,}') \n",
    "\n",
    "# Initialize objects for reuse\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b5cdf3-10c9-4fa9-ba35-f59835673b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_training_data(claims_file_path, evidence_file_path):\n",
    "    \"\"\"\n",
    "    Loads claims and evidence data from specified JSON files, flattens the evidence list in the claims data,\n",
    "    and merges the claims with their corresponding evidence based on evidence IDs. Returns a DataFrame ready for training.\n",
    "\n",
    "    Parameters:\n",
    "    - claims_file_path: str, path to the JSON file containing claims data.\n",
    "    - evidence_file_path: str, path to the JSON file containing evidence data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A Pandas DataFrame ready for training, where each row represents a claim-evidence pair.\n",
    "    \"\"\"\n",
    "    # Load and flatten claims data\n",
    "    claims_df = pd.read_json(claims_file_path, orient='index')\n",
    "    claims_df['evidences'] = claims_df.agg(lambda df: df['evidences'] if isinstance(df['evidences'], list) else [df['evidences']], axis=1)\n",
    "    claims_df = claims_df.explode('evidences')\n",
    "    claims_df.reset_index(inplace=True)\n",
    "    claims_df.columns = ['claim', 'claim_text', 'claim_label', 'evidence']\n",
    "    \n",
    "    # Load evidence data\n",
    "    evidence_df = pd.read_json(evidence_file_path, orient='index')\n",
    "    evidence_df.reset_index(inplace=True)\n",
    "    evidence_df.columns = ['evidence', 'evidence_text']\n",
    "    \n",
    "    # Merge claims and evidence data\n",
    "    train_df = pd.merge(claims_df, evidence_df, on='evidence', how='inner')\n",
    "    train_df['claim'] = train_df.agg(lambda df: int(df['claim'].split('-')[1]), axis=1)\n",
    "    train_df['evidence'] = train_df.agg(lambda df: int(df['evidence'].split('-')[1]), axis=1)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbf0664-3896-4094-851f-1e253df5c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses the given text.\"\"\"\n",
    "    try:\n",
    "        # Expand contractions\n",
    "        text = contractions.fix(text)\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove specific characters like '\"' and \"'\"\n",
    "    # Remove punctuation\n",
    "    text = specific_chars_pattern.sub('', text)\n",
    "    text = punctuation_pattern.sub(\"\", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize each token based on its POS tag\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in nltk.pos_tag(tokens):\n",
    "        wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "        lemmatized_token = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "\n",
    "    text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = multi_space_pattern.sub(' ', text)\n",
    "\n",
    "    # Strip leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3b0813-14a4-4afb-977f-ff34bdbfee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_text</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>evidence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1426</td>\n",
       "      <td>Many of the world’s coral reefs are already ba...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>288294</td>\n",
       "      <td>Aquaculture is showing promise as a potentiall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1426</td>\n",
       "      <td>Many of the world’s coral reefs are already ba...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>946262</td>\n",
       "      <td>This can rapidly result in transitions to barr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>698</td>\n",
       "      <td>A recent study led by Lawrence Livermore Natio...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>660755</td>\n",
       "      <td>A 2007 study by David Douglass and coworkers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1021</td>\n",
       "      <td>The corals may save themselves, as many other ...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>242575</td>\n",
       "      <td>The poleward migration of coral species refers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1021</td>\n",
       "      <td>The corals may save themselves, as many other ...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>1175280</td>\n",
       "      <td>One way, however, that corals \"might escape oc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim                                         claim_text  \\\n",
       "486   1426  Many of the world’s coral reefs are already ba...   \n",
       "487   1426  Many of the world’s coral reefs are already ba...   \n",
       "488    698  A recent study led by Lawrence Livermore Natio...   \n",
       "489   1021  The corals may save themselves, as many other ...   \n",
       "490   1021  The corals may save themselves, as many other ...   \n",
       "\n",
       "         claim_label  evidence  \\\n",
       "486  NOT_ENOUGH_INFO    288294   \n",
       "487  NOT_ENOUGH_INFO    946262   \n",
       "488          REFUTES    660755   \n",
       "489         SUPPORTS    242575   \n",
       "490         SUPPORTS   1175280   \n",
       "\n",
       "                                         evidence_text  \n",
       "486  Aquaculture is showing promise as a potentiall...  \n",
       "487  This can rapidly result in transitions to barr...  \n",
       "488  A 2007 study by David Douglass and coworkers, ...  \n",
       "489  The poleward migration of coral species refers...  \n",
       "490  One way, however, that corals \"might escape oc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage example\n",
    "claims_file_path = '../data/dev-claims.json'\n",
    "evidence_file_path = '../data/evidence.json'\n",
    "train_df = load_and_prepare_training_data(claims_file_path, evidence_file_path)\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1789254-dcf0-47c9-92f9-e932ef3ddb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_text</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>evidence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1426</td>\n",
       "      <td>many world coral reef already barren state con...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>288294</td>\n",
       "      <td>aquaculture show promise potentially effective...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1426</td>\n",
       "      <td>many world coral reef already barren state con...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>946262</td>\n",
       "      <td>rapidly result transition barren landscape rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>698</td>\n",
       "      <td>recent study lead lawrence livermore national ...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>660755</td>\n",
       "      <td>2007 study david douglas coworkers conclude 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1021</td>\n",
       "      <td>coral may save many creature attempt move towa...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>242575</td>\n",
       "      <td>poleward migration coral specie refers phenome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1021</td>\n",
       "      <td>coral may save many creature attempt move towa...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>1175280</td>\n",
       "      <td>one way however coral might escape ocean warm ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim                                         claim_text  \\\n",
       "486   1426  many world coral reef already barren state con...   \n",
       "487   1426  many world coral reef already barren state con...   \n",
       "488    698  recent study lead lawrence livermore national ...   \n",
       "489   1021  coral may save many creature attempt move towa...   \n",
       "490   1021  coral may save many creature attempt move towa...   \n",
       "\n",
       "         claim_label  evidence  \\\n",
       "486  NOT_ENOUGH_INFO    288294   \n",
       "487  NOT_ENOUGH_INFO    946262   \n",
       "488          REFUTES    660755   \n",
       "489         SUPPORTS    242575   \n",
       "490         SUPPORTS   1175280   \n",
       "\n",
       "                                         evidence_text  \n",
       "486  aquaculture show promise potentially effective...  \n",
       "487  rapidly result transition barren landscape rel...  \n",
       "488  2007 study david douglas coworkers conclude 22...  \n",
       "489  poleward migration coral specie refers phenome...  \n",
       "490  one way however coral might escape ocean warm ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_processed = train_df.copy()\n",
    "train_df_processed.claim_text = train_df_processed.agg(lambda df: preprocess_text(df.claim_text), axis=1)\n",
    "train_df_processed.evidence_text = train_df_processed.agg(lambda df: preprocess_text(df.evidence_text), axis=1)\n",
    "train_df_processed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e131358-abec-4ea5-a8b4-d0a0bcbe4394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cd194e3cbc40d4b0ea17a2f83ad25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208827 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evidence</th>\n",
       "      <th>evidence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1208822</th>\n",
       "      <td>evidence-1208822</td>\n",
       "      <td>also property contribute garage apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208823</th>\n",
       "      <td>evidence-1208823</td>\n",
       "      <td>class fn org fyrde 6110 volda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208824</th>\n",
       "      <td>evidence-1208824</td>\n",
       "      <td>dragon storm game roleplay game collectible ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208825</th>\n",
       "      <td>evidence-1208825</td>\n",
       "      <td>state zeriuani great realm tradition relate tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208826</th>\n",
       "      <td>evidence-1208826</td>\n",
       "      <td>storyline revolve around giant plesiosaur akin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 evidence                                      evidence_text\n",
       "1208822  evidence-1208822          also property contribute garage apartment\n",
       "1208823  evidence-1208823                      class fn org fyrde 6110 volda\n",
       "1208824  evidence-1208824  dragon storm game roleplay game collectible ca...\n",
       "1208825  evidence-1208825  state zeriuani great realm tradition relate tr...\n",
       "1208826  evidence-1208826  storyline revolve around giant plesiosaur akin..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evidence_df = pd.read_json('../data/evidence.json', orient='index')\n",
    "# evidence_df.reset_index(inplace=True)\n",
    "# evidence_df.columns = ['evidence', 'evidence_text']\n",
    "# evidence_df['evidence_text'] = evidence_df['evidence_text'].progress_apply(preprocess_text)\n",
    "# evidence_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1220ff56-6ae4-46c5-b3ea-7c866a9a69e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evidence</th>\n",
       "      <th>evidence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evidence-0</td>\n",
       "      <td>john bennet lawes english entrepreneur agricul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evidence-1</td>\n",
       "      <td>lindberg begin professional career age 16 even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evidence-2</td>\n",
       "      <td>boston lady cambridge vampire weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence-3</td>\n",
       "      <td>gerald francis goyer born october 20 1936 prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>evidence-4</td>\n",
       "      <td>detect abnormality oxytocinergic function schi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     evidence                                      evidence_text\n",
       "0  evidence-0  john bennet lawes english entrepreneur agricul...\n",
       "1  evidence-1  lindberg begin professional career age 16 even...\n",
       "2  evidence-2              boston lady cambridge vampire weekend\n",
       "3  evidence-3  gerald francis goyer born october 20 1936 prof...\n",
       "4  evidence-4  detect abnormality oxytocinergic function schi..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = '../data/processed_evidence.csv'\n",
    "# evidence_df.to_csv(output_path, index=False)\n",
    "evidence_df = pd.read_csv(output_path, dtype={'evidence_text': str}).dropna()\n",
    "evidence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f07136d7-3eb0-4c57-90e6-2422f1a8fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming preprocessing is done, directly using 'evidence_text'\n",
    "evidence_corpus = evidence_df['evidence_text']\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the evidence corpus\n",
    "tfidf_matrix_evidence = vectorizer.fit_transform(evidence_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbe0c776-7250-44aa-bd8c-8263fb6e8429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6976016 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(train_df_processed['claim_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c480ca3-c0fc-4882-bccf-01926f7e626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming `train_df_processed` is your DataFrame and already loaded\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Combine claim_text and evidence_text for vectorization\n",
    "all_texts = pd.concat([train_df_processed['claim_text'], train_df_processed['evidence_text']])\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "# Transform texts\n",
    "claims_tfidf = vectorizer.transform(train_df_processed['claim_text'])\n",
    "evidence_tfidf = vectorizer.transform(train_df_processed['evidence_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05335cf7-f6ae-4d9b-b6d8-24c777172f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbeac8-595a-42ea-8595-3d1a5d343a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "447012c6-cb83-4808-9b04-be59c59feb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b83420cf-75fc-4987-a57c-327af6422a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels=None):\n",
    "        self.claims = claims\n",
    "        self.evidences = evidences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.claims[idx], self.evidences[idx], self.labels[idx]\n",
    "        return self.claims[idx], self.evidences[idx]\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/dev-train.csv')\n",
    "\n",
    "# Preprocess and split data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "claims_tfidf = vectorizer.fit_transform(df['claim_text']).toarray()\n",
    "evidences_tfidf = vectorizer.transform(df['evidence_text']).toarray()\n",
    "labels = df['evidence'].values\n",
    "\n",
    "X_train_claims, X_test_claims, X_train_evidences, X_test_evidences, y_train, y_test = train_test_split(\n",
    "    claims_tfidf, evidences_tfidf, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = ClaimEvidenceDataset(X_train_claims, X_train_evidences, y_train)\n",
    "test_dataset = ClaimEvidenceDataset(X_test_claims, X_test_evidences, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28229f7a-0ad2-4407-82fe-c09bb77364ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dropout(hn[-1])\n",
    "        return x\n",
    "\n",
    "class FactCheckFNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FactCheckFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4fe05af-d43f-4e6c-bf07-6e92af2e6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, early_stopping_tolerance=0.0001):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for claims, evidences, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            optimizer.zero_grad()\n",
    "            # Ensure claims are tensors of the correct dtype and on the correct device\n",
    "            claims_tensor = claims.float()\n",
    "            # Adjust the labels tensor to match the output dimensions and dtype\n",
    "            labels_tensor = labels.float().unsqueeze(1)\n",
    "            outputs = model(claims_tensor)\n",
    "            loss = criterion(outputs, labels_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}, Val Loss: {val_loss}')\n",
    "\n",
    "        if val_loss < best_val_loss - early_stopping_tolerance:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > 3:  # Stop after 3 epochs without improvement\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for claims, evidences, labels in loader:\n",
    "            # Ensure claims are tensors of the correct dtype and on the correct device\n",
    "            claims_tensor = claims.float()\n",
    "            # Adjust the labels tensor to match the output dimensions and dtype\n",
    "            labels_tensor = labels.float().unsqueeze(1)\n",
    "            outputs = model(claims_tensor)\n",
    "            loss = criterion(outputs, labels_tensor)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3dcec-a130-4539-8bf1-d06753fdb7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming the FNN model is for binary classification\n",
    "fnn_model = FactCheckFNN(input_dim=X_train_claims.shape[1], hidden_dim=128, output_dim=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(fnn_model.parameters())\n",
    "\n",
    "\n",
    "train_model(fnn_model, train_loader, test_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "824eeb78-0c0c-4722-916f-5edc03aed853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevant_evidence(claim_tfidf, evidence_tfidf_matrix, threshold=0.45, max_outputs=6):\n",
    "    similarity_scores = cosine_similarity(claim_tfidf.reshape(1, -1), evidence_tfidf_matrix)[0]\n",
    "    relevant_indices = np.where(similarity_scores >= threshold)[0]\n",
    "    \n",
    "    if len(relevant_indices) == 0:\n",
    "        most_relevant_index = np.argmax(similarity_scores)\n",
    "        return [f'evidence-{most_relevant_index}']\n",
    "    else:\n",
    "        sorted_indices = np.argsort(similarity_scores[relevant_indices])[::-1]\n",
    "        return [f'evidence-{idx}' for idx in sorted_indices[:max_outputs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad509d00-1598-49c7-9c7d-fd2befb57b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evidence-1', 'evidence-0']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "claim_index = 0  # Index of the claim to test\n",
    "predicted_evidences = predict_relevant_evidence(X_test_claims[claim_index], X_train_evidences)\n",
    "print(predicted_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a4d5f-ccc5-4540-90a4-fab9e9105afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75abda-28a0-4850-8ed6-b398f6424376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152e742-ba0e-47b9-b45c-a3dfc96effdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3f12b6b-9298-4503-8547-fd548961599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "# Initialize TF-IDF Vectorizer without specifying max_features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the claim texts\n",
    "claims_tfidf = vectorizer.fit_transform(df['claim_text']).toarray()\n",
    "\n",
    "# Dynamically determine the actual size of TF-IDF vectors\n",
    "input_dim = claims_tfidf.shape[1]\n",
    "\n",
    "# Prepare TF-IDF for claims\n",
    "claims_tfidf = vectorizer.fit_transform(df['claim_text']).toarray()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(claims_tfidf, df['evidence'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Dataset\n",
    "class TFIDFDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "train_dataset = TFIDFDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TFIDFDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eac34f99-5303-4e40-836a-e084d23e5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to [batch_size, 1, input_dim] to represent a sequence of length 1\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model with the correct input_dim\n",
    "model = LSTMClassifier(input_dim=input_dim, hidden_dim=128, output_dim=len(np.unique(df['evidence'])), num_layers=2, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0597c2c9-8008-44fd-b8f4-4c427ef140c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 797867 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, early_stopping_tolerance)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-group-project/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-group-project/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-group-project/lib/python3.8/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-group-project/lib/python3.8/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 797867 is out of bounds."
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=100, early_stopping_tolerance=0.0001):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "        # Implement early stopping logic here based on validation loss\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af87721-e796-4e28-b611-2bb125467f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d2f5b-0568-4aa8-aed0-20f9a8f863e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a3175-ae04-44f8-8b63-82c65c7f649a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf5e72-7eed-4c14-bc9b-cead9e999100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df134b1d-ce07-42ae-8b1e-630e808ce511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the entire 'evidence' column before splitting\n",
    "df['evidence_encoded'] = label_encoder.fit_transform(df['evidence'])\n",
    "\n",
    "# Now, when you split the data, use 'evidence_encoded' as your labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['claim_text'], df['evidence_encoded'], test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "658d2700-c112-4507-974c-c1827fed32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=50000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35456474-0eb8-4d7d-b8d7-45ebecd904a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452               another global warming myth come crash\n",
       "84     late ipcc report ar5 show global mean temperat...\n",
       "434    unlikely scenario sudden become probable thoug...\n",
       "474    ipcc report warn last week world “ nowhere nea...\n",
       "428    protect restore forest would reduce 18 emissio...\n",
       "Name: claim_text, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c47b639f-da6f-4719-bac1-07a597f89bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Simple text cleaning (for demonstration; consider more thorough preprocessing)\n",
    "df['claim_text'] = df['claim_text'].str.lower()\n",
    "df['evidence_text'] = df['evidence_text'].str.lower()\n",
    "\n",
    "# Split dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[['claim_text', 'evidence_text']], df['evidence'], test_size=0.01, random_state=42)\n",
    "\n",
    "# Tokenize text and build vocabulary\n",
    "vocab = Counter()\n",
    "for text in pd.concat([train_texts['claim_text'], train_texts['evidence_text']]):\n",
    "    vocab.update(text.split())\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx+1 for idx, (word, _) in enumerate(vocab.items())}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Function to encode texts\n",
    "def encode_text(text, word_to_idx):\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "# Custom Dataset class\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels, word_to_idx):\n",
    "        self.claims = [encode_text(claim, word_to_idx) for claim in claims]\n",
    "        self.evidences = [encode_text(evidence, word_to_idx) for evidence in evidences]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.claims[idx]), torch.tensor(self.evidences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = ClaimEvidenceDataset(train_texts['claim_text'], train_texts['evidence_text'], train_labels, word_to_idx)\n",
    "test_dataset = ClaimEvidenceDataset(test_texts['claim_text'], test_texts['evidence_text'], test_labels, word_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8da90121-0ef7-4ccf-9fc5-790754471901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        out = self.fc(hidden)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b87f86f1-6414-4acf-8471-5033bcce3c95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m texts, _, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Here you'd need to adjust the data handling to fit the LSTM input requirements\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m model(texts)\n\u001b[1;32m     18\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1  # Change according to your task, e.g., binary classification\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMModel(vocab_size + 1, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Example epoch count\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for texts, _, labels in train_loader:\n",
    "        # Here you'd need to adjust the data handling to fit the LSTM input requirements\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "758c12cd-99c7-4285-b257-64296408f8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: -675.6174755096436\n",
      "Epoch 2, Loss: -2623.859085083008\n",
      "Epoch 3, Loss: -5114.236389160156\n",
      "Epoch 4, Loss: -7488.131774902344\n",
      "Epoch 5, Loss: -10036.855499267578\n",
      "Epoch 1, Loss: -11897.787048339844\n",
      "Epoch 2, Loss: -13246.202209472656\n",
      "Epoch 3, Loss: -14702.699890136719\n",
      "Epoch 4, Loss: -15765.975280761719\n",
      "Epoch 5, Loss: -16069.664428710938\n",
      "Validation Loss: -10508.384765625\n",
      "Top Probabilities: tensor([0.4876])\n",
      "Evidence IDs: [0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Simple text cleaning\n",
    "df['claim_text'] = df['claim_text'].str.lower().fillna('')\n",
    "df['evidence_text'] = df['evidence_text'].str.lower().fillna('')\n",
    "\n",
    "# Ensure labels are numeric if they are not already\n",
    "label_mapping = {'true': 1, 'false': 0}  # Adjust according to your data\n",
    "df['claim_label'] = df['claim']\n",
    "\n",
    "# Split dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[['claim_text', 'evidence_text']], df['claim_label'], test_size=0.01, random_state=42)\n",
    "\n",
    "# Tokenize text and build vocabulary\n",
    "vocab = Counter()\n",
    "for text in pd.concat([train_texts['claim_text'], train_texts['evidence_text']]):\n",
    "    vocab.update(text.split())\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx+1 for idx, (word, _) in enumerate(vocab.items())}\n",
    "\n",
    "# Function to encode texts\n",
    "def encode_text(text, word_to_idx):\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "# Custom Dataset class\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels, word_to_idx):\n",
    "        self.claims = [encode_text(claim, word_to_idx) for claim in claims]\n",
    "        self.evidences = [encode_text(evidence, word_to_idx) for evidence in evidences]\n",
    "        self.labels = labels.to_numpy().astype(float)  # Ensure labels are float\n",
    "        self.indices = np.arange(len(labels))  # Keep track of indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.claims[idx], self.evidences[idx], self.labels[idx], self.indices[idx]\n",
    "\n",
    "# collate_fn function\n",
    "def collate_batch(batch):\n",
    "    claim_list, evidence_list, label_list, indices_list = [], [], [], []\n",
    "    for (claim, evidence, label, idx) in batch:\n",
    "        claim_tensor = torch.tensor(claim, dtype=torch.long)\n",
    "        evidence_tensor = torch.tensor(evidence, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        claim_list.append(claim_tensor)\n",
    "        evidence_list.append(evidence_tensor)\n",
    "        label_list.append(label_tensor)\n",
    "        indices_list.append(idx)\n",
    "    claim_list = pad_sequence(claim_list, batch_first=True, padding_value=0)\n",
    "    evidence_list = pad_sequence(evidence_list, batch_first=True, padding_value=0)\n",
    "    label_list = torch.stack(label_list)\n",
    "    return claim_list, evidence_list, label_list, torch.tensor(indices_list, dtype=torch.long)\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = ClaimEvidenceDataset(train_texts['claim_text'], train_texts['evidence_text'], train_labels, word_to_idx)\n",
    "test_dataset = ClaimEvidenceDataset(test_texts['claim_text'], test_texts['evidence_text'], test_labels, word_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMModel(vocab_size + 1, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop with gradient clipping\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for claims, _, labels, _ in train_loader:\n",
    "        predictions = model(claims).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation function corrected to handle four items\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # Corrected to unpack four items\n",
    "        for claims, _, labels, _ in data_loader:\n",
    "            predictions = model(claims).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Training loop corrected for unpacking four items\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Corrected to unpack four items\n",
    "    for claims, _, labels, _ in train_loader:\n",
    "        predictions = model(claims).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Predict function with evidence_id retrieval updated to return up to 6 results\n",
    "def predict_with_evidence_id(model, claims, dataset):\n",
    "    model.eval()\n",
    "    claims_encoded = [encode_text(claim, word_to_idx) for claim in claims]\n",
    "    claims_encoded = pad_sequence([torch.tensor(claim) for claim in claims_encoded], batch_first=True, padding_value=0)\n",
    "    evidence_ids = []\n",
    "    with torch.no_grad():\n",
    "        predictions = model(claims_encoded).squeeze(1)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        top_prob, top_idx = torch.topk(probabilities, k=min(6, len(claims))) \n",
    "        for idx in top_idx:\n",
    "            _, _, _, evidence_id = dataset[idx.item()]\n",
    "            evidence_ids.append(evidence_id)\n",
    "\n",
    "\n",
    "    return top_prob, evidence_ids\n",
    "\n",
    "# Example of using the evaluation function\n",
    "val_loss = evaluate(model, test_loader)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "# Example of using the predict function\n",
    "claims_new = [\"[South Australia] has the most expensive electricity in the world.\"]\n",
    "top_probabilities, evidence_ids = predict_with_evidence_id(model, claims_new, test_dataset)\n",
    "print(f\"Top Probabilities: {top_probabilities}\")\n",
    "print(f\"Evidence IDs: {evidence_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1373d1d-e8cd-44fd-add8-f48503cd17d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_text</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>evidence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>752</td>\n",
       "      <td>south australia expensive electricity world</td>\n",
       "      <td>752</td>\n",
       "      <td>67732</td>\n",
       "      <td>citation need south australia high retail pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530</td>\n",
       "      <td>south australia win unreliable grid world outs...</td>\n",
       "      <td>530</td>\n",
       "      <td>67732</td>\n",
       "      <td>citation need south australia high retail pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>752</td>\n",
       "      <td>south australia expensive electricity world</td>\n",
       "      <td>752</td>\n",
       "      <td>572512</td>\n",
       "      <td>south australia high power price world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>375</td>\n",
       "      <td>3 per cent total annual global emission carbon...</td>\n",
       "      <td>375</td>\n",
       "      <td>996421</td>\n",
       "      <td>2011 unep green economy report state aagricult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375</td>\n",
       "      <td>3 per cent total annual global emission carbon...</td>\n",
       "      <td>375</td>\n",
       "      <td>1080858</td>\n",
       "      <td>market share 30 potentially clean electricity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1426</td>\n",
       "      <td>many world coral reef already barren state con...</td>\n",
       "      <td>1426</td>\n",
       "      <td>288294</td>\n",
       "      <td>aquaculture show promise potentially effective...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1426</td>\n",
       "      <td>many world coral reef already barren state con...</td>\n",
       "      <td>1426</td>\n",
       "      <td>946262</td>\n",
       "      <td>rapidly result transition barren landscape rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>698</td>\n",
       "      <td>recent study lead lawrence livermore national ...</td>\n",
       "      <td>698</td>\n",
       "      <td>660755</td>\n",
       "      <td>2007 study david douglas coworkers conclude 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1021</td>\n",
       "      <td>coral may save many creature attempt move towa...</td>\n",
       "      <td>1021</td>\n",
       "      <td>242575</td>\n",
       "      <td>poleward migration coral specie refers phenome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1021</td>\n",
       "      <td>coral may save many creature attempt move towa...</td>\n",
       "      <td>1021</td>\n",
       "      <td>1175280</td>\n",
       "      <td>one way however coral might escape ocean warm ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim                                         claim_text  claim_label  \\\n",
       "0      752        south australia expensive electricity world          752   \n",
       "1      530  south australia win unreliable grid world outs...          530   \n",
       "2      752        south australia expensive electricity world          752   \n",
       "3      375  3 per cent total annual global emission carbon...          375   \n",
       "4      375  3 per cent total annual global emission carbon...          375   \n",
       "..     ...                                                ...          ...   \n",
       "486   1426  many world coral reef already barren state con...         1426   \n",
       "487   1426  many world coral reef already barren state con...         1426   \n",
       "488    698  recent study lead lawrence livermore national ...          698   \n",
       "489   1021  coral may save many creature attempt move towa...         1021   \n",
       "490   1021  coral may save many creature attempt move towa...         1021   \n",
       "\n",
       "     evidence                                      evidence_text  \n",
       "0       67732  citation need south australia high retail pric...  \n",
       "1       67732  citation need south australia high retail pric...  \n",
       "2      572512             south australia high power price world  \n",
       "3      996421  2011 unep green economy report state aagricult...  \n",
       "4     1080858  market share 30 potentially clean electricity ...  \n",
       "..        ...                                                ...  \n",
       "486    288294  aquaculture show promise potentially effective...  \n",
       "487    946262  rapidly result transition barren landscape rel...  \n",
       "488    660755  2007 study david douglas coworkers conclude 22...  \n",
       "489    242575  poleward migration coral specie refers phenome...  \n",
       "490   1175280  one way however coral might escape ocean warm ...  \n",
       "\n",
       "[491 rows x 5 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da72bb-04f2-46c4-8f22-f605ba456925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.8.19",
   "language": "python",
   "name": "python3.8.19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
