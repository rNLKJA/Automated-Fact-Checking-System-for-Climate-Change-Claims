{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We use Python 3.8 for all labs in this subject.*\n",
    "\n",
    "Train BPE on a toy text example\n",
    "\n",
    "bpe algorithm: https://web.stanford.edu/~jurafsky/slp3/2.pdf (2.4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "text = \"The aims for this subject is for students to develop an understanding of the main algorithms used in natural language processing, for use in a diverse range of applications including text classification, machine translation, and question answering. Topics to be covered include part-of-speech tagging, n-gram language modelling, syntactic parsing and deep learning. The programming language used is Python, see for more information on its use in the workshops, assignments and installation at home.\"\n",
    "# text = 'low '*5 +'lower '*2+'newest '*6 +'widest '*3\n",
    "def get_vocab(text):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word in text.strip().split():\n",
    "        #note: we use the special token </w> (instead of underscore in the lecture) to denote the end of a word\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This function iterates through all words in the vocabulary and count pair of tokens which are next to each other.\n",
    "\n",
    "EXAMPLE:\n",
    "    word = 'T h e <\\w>'\n",
    "    pairs of tokens in this word [('T', 'h'), ('h', 'e'), ('e', '<\\w>')]\n",
    "    \n",
    "INPUT:\n",
    "    vocab: Dict[str, int]  # The vocabulary, a counter for word frequency\n",
    "    \n",
    "OUTPUT:\n",
    "    pairs: Dict[Tuple[str, str], int] # Word pairs, a counter for pair frequency\n",
    "\n",
    "\"\"\"\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function merges a given pair of tokens in all words in the vocabulary\n",
    "\n",
    "EXAMPLE:\n",
    "    word = 'T h e <\\w>'\n",
    "    pair = ('e', '<\\w>')\n",
    "    word_after_merge = 'T h e<\\w>'\n",
    "    \n",
    "Input:\n",
    "    pair: Tuple[str, str] # the pair of tokens need to be merged\n",
    "    v_in: Dict[str, int]  # vocabulary before merge\n",
    "    \n",
    "Output:\n",
    "    v_out: Dict[str, int] # vocabulary after merge\n",
    "    \n",
    "HINT:\n",
    "    When merging pair ('h', 'e') for word 'Th e<\\w>', the two tokens in this word 'Th' and 'e<\\w>' shouldn't be merged.\n",
    "\n",
    "\"\"\"\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return v_out\n",
    "\n",
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "print(\"Vocab =\", vocab)\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')\n",
    "\n",
    "#about 100 merges we start to see common words\n",
    "num_merges = 100\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    new_token = ''.join(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    # add new token to the vocab\n",
    "    tokens[new_token] = pairs[best]\n",
    "    # deduct frequency for tokens have been merged\n",
    "    tokens[best[0]] -= pairs[best]\n",
    "    tokens[best[1]] -= pairs[best]\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, used the BPE dictionaries to tokenise sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab_tokenization(vocab):\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "    \n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token] * len(string)\n",
    "\n",
    "    string_tokens = []\n",
    "    # iterate over all tokens to find match\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token)\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # if no match found in the string, go to next token\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        # collect end position of matches in the string\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # slice for sub-word\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # tokenize this sub-word with tokens remaining\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # tokenize the remaining string\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    else:\n",
    "        # return list of unknown token if no match is found for the string\n",
    "        string_tokens = [unknown_token] * len(string)\n",
    "        \n",
    "    return string_tokens\n",
    "\n",
    "\"\"\"\n",
    "This function generates a list of all tokens sorted by their length (1st key) and frequency (2nd key).\n",
    "\n",
    "EXAMPLE:\n",
    "    token frequency dictionary before sorting: {'natural': 3, 'language':2, 'processing': 4, 'lecture': 4}\n",
    "    sorted tokens: ['processing', 'language', 'lecture', 'natural']\n",
    "    \n",
    "INPUT:\n",
    "    token_frequencies: Dict[str, int] # Counter for token frequency\n",
    "    \n",
    "OUTPUT:\n",
    "    sorted_token: List[str] # Tokens sorted by length and frequency\n",
    "\n",
    "\"\"\"\n",
    "def sort_tokens(tokens_frequencies):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return sorted_tokens\n",
    "\n",
    "#display the vocab\n",
    "vocab_tokenization = get_vocab_tokenization(vocab)\n",
    "\n",
    "#sort tokens by length and frequency\n",
    "sorted_tokens = sort_tokens(tokens)\n",
    "print(\"Tokens =\", sorted_tokens, \"\\n\")\n",
    "\n",
    "sentence_1 = 'I like natural language processing!'\n",
    "sentence_2 = 'I like natural languaaage processing!'\n",
    "sentence_list = [sentence_1, sentence_2]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    \n",
    "    print('==========')\n",
    "    print(\"Sentence =\", sentence)\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        word = word + \"</w>\"\n",
    "\n",
    "        print('Tokenizing word: {}...'.format(word))\n",
    "        if word in vocab_tokenization:\n",
    "            print(vocab_tokenization[word])\n",
    "        else:\n",
    "            print(tokenize_word(string=word, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
