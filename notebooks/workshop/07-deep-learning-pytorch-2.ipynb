{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144557d9-109a-4a6a-9143-86edebb449f0",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc18c86-b744-4785-9141-121287129aad",
   "metadata": {},
   "source": [
    "In this workshop, we will try to build some feedforward models to do sentiment analysis, using pytorch, a deep learning library: https://pytorch.org/\n",
    "\n",
    "You will need pandas, torch to run this code (pip install pandas torch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b210e2-662b-47c2-8c95-60b8fbd5529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas scikit-learn torch torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81e308-447f-441e-a15e-24dc3505d347",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb05780-9a04-48c6-a160-a19359b40b45",
   "metadata": {},
   "source": [
    "First let's prepare the data. We are using 1000 yelp reviews, nnotated with either positive or negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabcb7e7-3deb-4236-845d-fc6a833858cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences = 1000\n",
      "\n",
      "Data:\n",
      "                                    sentence  label\n",
      "0                   Wow... Loved this place.      1\n",
      "1                         Crust is not good.      0\n",
      "2  Not tasty and the texture was just nasty.      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = \"07-yelp-dataset.txt\"\n",
    "df = pd.read_csv(corpus, names=['sentence', 'label'], sep='\\t')\n",
    "print(\"Number of sentences =\", len(df))\n",
    "print(\"\\nData:\")\n",
    "print(df.iloc[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12901d-daec-4a76-972a-5682efa29725",
   "metadata": {},
   "source": [
    "Next, let's create the train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b79035-b049-40e8-bfc2-dbf080ff9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Wow... Loved this place.\n",
      "0 I'm super pissd.\n",
      "0 Spend your money elsewhere.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "sentences = df['sentence'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "#partition data into 80/10/10 for train/dev/test\n",
    "sentences_train, y_train = sentences[:800], labels[:800]\n",
    "sentences_dev, y_dev = sentences[800:900], labels[800:900]\n",
    "sentences_test, y_test = sentences[900:1000], labels[900:1000]\n",
    "\n",
    "#convert label list into arrays\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0], sentences_train[0])\n",
    "print(y_dev[0], sentences_dev[0])\n",
    "print(y_test[0], sentences_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f556a3-1e50-487e-8026-186ec505725a",
   "metadata": {},
   "source": [
    "## Building vocabulary set and vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a2d09-640f-493d-8058-5cf4f97e3df2",
   "metadata": {},
   "source": [
    "In this workshop, we will employ the `tokenizer` function provided by PyTorch to process our data. After tokenization, the next step involves using `build_vocab_from_iterator` to construct a frequency dictionary for our vocabulary. Moreover, we incorporate two special tokens, `<unk>` and `<pad>`, into our vocabulary set. The `<unk>` token is designated for managing tokens that have not been seen during training, ensuring the model can handle new or rare words. The `<pad>` token, on the other hand, will be utilized later to pad sequences of varying lengths, allowing us to standardize them to a uniform length for model processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50b0d6f-6fc6-47e0-bc83-e948340cdc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingbin/anaconda3/envs/gpu_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = sentences_train\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "padding_index = vocab[\"<pad>\"]\n",
    "\n",
    "# Input our just built vocab list and tokenizer, \n",
    "# we build a CountVectorizer for Bag of Word representation transformation\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer, vocabulary=vocab.get_stoi(), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d307b754-2034-429b-8e45-bae5ff3935b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(sentences_train).toarray() #BOW representation\n",
    "x_dev = vectorizer.transform(sentences_dev).toarray() #BOW representation\n",
    "x_test = vectorizer.transform(sentences_test).toarray() #BOW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e265d9-17c3-4a99-be69-00f018604072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1815)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507118e5-2396-4177-855e-87d9b7290dd1",
   "metadata": {},
   "source": [
    "Now every sentence has been transformed into a vector with frequency count of 1814 vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab72774d-c736-4d25-9473-c66b8a7c04eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 1815\n",
      "['wow', '.', '.', '.', 'loved', 'this', 'place', '.']\n",
      "[0 0 4 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = x_train.shape[1]\n",
    "print(\"Vocab size =\", vocab_size)\n",
    "print(tokenizer(sentences_train[0]))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173786a-548c-4c7c-b2ed-be805b44af32",
   "metadata": {},
   "source": [
    "## Baseline with sklearn logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266956f-6a9a-4edf-9c4b-fdf9cdf4ae98",
   "metadata": {},
   "source": [
    "Before we build a neural network model, let's see how well logistic regression do with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc08fed4-29ce-45ad-a680-b559c45eba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9a97e-c02f-4c1c-ad52-009cb90e66f3",
   "metadata": {},
   "source": [
    "The logistic regression result is not too bad, and it will serve as a baseline for the deep learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c647d-4b0b-428b-ba60-81968920bd41",
   "metadata": {},
   "source": [
    "## Short Introduction of Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b6ebe-7af2-490e-9ccd-0a1a634d1467",
   "metadata": {},
   "source": [
    "PyTorch is an open-source machine learning, especially deep learning, library widely acclaimed for its flexibility, speed, and ease of use. To learn pytorch properly please refer to their official tutorial: https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7047a-43e7-4900-86e6-7ab786222d1b",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cab6f-18bc-4992-b7e8-42abe5e94fb8",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to numpy arrays and matrices. But more than numpy array, tensors cache and trace  the mathematic operation that the been carried; therefore capable of automatically calculate the gradient for back propagation during training. Beyond mere numerical storage, tensors uniquely track and record the mathematical operations performed on them. This intrinsic capability allows for the automatic computation of gradients, a crucial component for the backpropagation process during neural network training. When operating with your tensors, there are three important properties to look after, which are \"shape (The dimenions of the matrices)\", \"dtype (the data type of the values)\", \"device (The harware (cpu, gpu) the values are stored)\". detail please see: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59553894-acf3-42a7-bb30-edab067899b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44948700-cde0-41c8-88d0-ba2d365f16b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870fe5c-ffe3-49de-ada2-65d4e2056700",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f09ff5-eeca-464d-8d84-7e8335fe8a8c",
   "metadata": {},
   "source": [
    "For efficient training of deep learning models, it is common practice to train in batches rather than processing instances individually. Moreover, it is essential to convert the data into PyTorch tensors prior to training. Given that unstructured data, including text and images, often necessitates pre-processing, establishing a data pipeline for pre-processing becomes imperative for effective model training. This pipeline not only streamlines the preparation of data but also ensures compatibility with PyTorch's computational framework. For a detailed guide, please refer to the official tutorial: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c321894-a34a-424b-9ac7-6f830305b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "def bow_collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text)\n",
    "    # For each batched data, we convert the values into tensor.\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.float32)\n",
    "\n",
    "    # We also place each tensor to an assigned device\n",
    "    return text_list.to(device), label_list.reshape(-1, 1).to(device)\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(list(zip(x_train, y_train)), batch_size=batch_size, collate_fn=bow_collate_batch)\n",
    "dev_dataloader = DataLoader(list(zip(x_dev, y_dev)), batch_size=batch_size, collate_fn=bow_collate_batch)\n",
    "test_dataloader = DataLoader(list(zip(x_test, y_test)), batch_size=batch_size, collate_fn=bow_collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ac380-1689-4e16-b450-85f9ce69291e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee88a7b-6f44-4246-a159-36e1bb0f3edb",
   "metadata": {},
   "source": [
    "PyTorch neural network models consist of sequential layers, each containing parameters that the network learns from during training. To create a custom model in PyTorch, your class should inherit from nn.Module, which is the base class for all neural network modules. Within the constructor method \\_\\_init\\_\\_, you can define the layers of your model. The forward propagation of the network, where the actual computation is performed, is defined in a method named forward within your class. This method specifies how data passes through the model. Below is an example of how a simple network for a Bag of Words model might be structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0ce201-d0c2-4a91-a8b0-de3c8fdf7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BowNetwork(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Linear(vocab_size, hidden_dim)\n",
    "        # Each layer has been randomly initaised when first created.\n",
    "        # You can further initialise the weight with different algorithms like below.\n",
    "        # torch.nn.init.uniform_(self.first_layer.weight)\n",
    "        self.second_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, vocab_size]\n",
    "        \n",
    "        x = torch.relu(self.first_layer(x))\n",
    "        # x.shape = [batch_size, hidden_size]\n",
    "        \n",
    "        logits = torch.sigmoid(self.second_layer(x))\n",
    "        # logits.shape = [batch_size, 1]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4bbde72-4f80-4f95-bf44-a2fb2579eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BowNetwork(\n",
      "  (first_layer): Linear(in_features=1815, out_features=10, bias=True)\n",
      "  (second_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bow_model = BowNetwork(vocab_size, 10).to(device)\n",
    "print(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c219b-ab41-4bfb-8282-b11e1d8d8540",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0eab51-9e35-4eaa-afce-8dce8625980b",
   "metadata": {},
   "source": [
    "Training a neural network is an iterative process that involves updating the model's weights to optimize performance on a given task. This process unfolds over multiple iterations, known as epochs, during which the model undergoes several key steps, as outlined below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76eaa9c-88aa-4e86-b35b-e77caee0eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Calling .train() will evoke the tensor to start caching steps and gradients.\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction by directly calling the model variable.\n",
    "        pred = model(X)\n",
    "\n",
    "        # Calculate the loss by comparing the prediction and the true labels.\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation: calculate the gradient by walking back the cached steps.\n",
    "        loss.backward()\n",
    "        # Update the parameters of the model with the loss gradient.\n",
    "        optimizer.step()\n",
    "        # Remove all the gradient to be ready for the next training of the next batch.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch  == size - 1:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef9681-4870-4d32-8306-0ee6271bac13",
   "metadata": {},
   "source": [
    "It's essential to specify our optimizer—a component that dictates the learning strategy through parameters such as the learning rate—and our loss function, which measures the discrepancy between the model's predictions and the actual data. Finally, we must determine the number of epochs, or complete passes through the training dataset, to effectively guide the training process to convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d125682a-517b-444f-b47a-3b03b545719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select Binacy cross entropy loss\n",
    "loss_fn = nn.BCELoss()\n",
    "# We select Adam optimizer and hook it up with our model parameters.\n",
    "optimizer = torch.optim.Adam(bow_model.parameters(), lr=0.001)\n",
    "# we set epochs to 30\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905fe52b-7539-4090-9470-1b8f2ebdbcfa",
   "metadata": {},
   "source": [
    "During each epoch iteration, we also would like to gauge the progress of the model's performance. We can use the dev set with a test functin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a21049-27d3-4c83-9ebf-9b5ba31e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # After calling eval(), the model no longer caching steps and gradient.\n",
    "    # The model also does the inference faster with less resource.\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    # This line specify that there will be no gradient in the operation below.\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            result = (pred>0.5).float()\n",
    "            correct += (result == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c53bef-6e43-40ac-a563-fd9b97011b0e",
   "metadata": {},
   "source": [
    "Now we can run our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4649c55a-48ed-45b0-96cc-5e3e8f671409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BOW feedforward network model!\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r2/7jlcp3b902q58h1xmcd_nrrm0000gp/T/ipykernel_15119/336960215.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  text_list = torch.tensor(text_list, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 0.687229 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.633776 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.572057 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.519992 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.479987 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.451103 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.431626 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.418669 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.410531 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.404651 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.402226 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.400482 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.400454 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.401982 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.403704 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.406832 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.409994 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.413807 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.418292 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.422736 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training BOW feedforward network model!\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, bow_model, loss_fn, optimizer)\n",
    "    test(dev_dataloader, bow_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89734ebc-af9a-439d-b72a-86d3953e8518",
   "metadata": {},
   "source": [
    "Now test it with the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56885144-b792-4be0-ba96-52421b6a22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test:\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.605542 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"final test:\" )\n",
    "test(test_dataloader, bow_model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c86b4a-7ecf-441a-b3ae-436591643d77",
   "metadata": {},
   "source": [
    "How does the performance compare to logistic regression? If you run it a few times you may find that it gives slightly different numbers, and that is due to random initialisation of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf7f23-3cdf-4dd1-8f04-bf558e281027",
   "metadata": {},
   "source": [
    "## Embedding cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26359a39-2d6b-41c7-81cb-f446f5b8a13c",
   "metadata": {},
   "source": [
    "Even though we did not explicitly define any word embeddings in the model architecture, they are in our model: in the weights between the input and the hidden layer. The hidden layer can therefore be interpreted as a sum of word embeddings for each input document.\n",
    "\n",
    "Let's fetch the word embeddings of some words, and look at their cosine similarity, and see if they make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3e8d52f-b989-4334-a381-8c96be080a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def display_embedding_similarity_examples(embeddings, vocab):\n",
    "    emb_love = embeddings[vocab[\"love\"]]  # embeddings for 'love'\n",
    "    emb_like = embeddings[vocab[\"like\"]]\n",
    "    emb_lukewarm = embeddings[vocab[\"lukewarm\"]]\n",
    "    emb_bad = embeddings[vocab[\"bad\"]]\n",
    "\n",
    "    print(\"show embedding similarity examples...\")\n",
    "    print(\"embedding vector of love:\")\n",
    "    print(emb_love)\n",
    "\n",
    "    print(\"embedding cosine similarity comparisons:\")\n",
    "    print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "    print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "    print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "    print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c22d266-e942-4d29-9729-54551e447b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show embedding of bow model\n",
      "show embedding similarity examples...\n",
      "embedding vector of love:\n",
      "[-0.24087326 -0.26665378  0.23608996 -0.20741412  0.25065786 -0.28107777\n",
      " -0.21571027  0.31935486  0.253168    0.27316606]\n",
      "embedding cosine similarity comparisons:\n",
      "love vs. like = 0.87393296\n",
      "love vs. lukewarm = -0.994362\n",
      "love vs. bad = -0.98634064\n",
      "lukewarm vs. bad = 0.9942637\n"
     ]
    }
   ],
   "source": [
    "print(\"show embedding of bow model\")\n",
    "# extract word embeddings layer\n",
    "embeddings = bow_model.first_layer.weight.T.to(\"cpu\").detach().numpy()\n",
    "display_embedding_similarity_examples(embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f5849-3eeb-4e56-b10e-c1fa05b2cace",
   "metadata": {},
   "source": [
    "Not bad. You should find that for *love* and *like*, which are both positive sentiment words, produce high cosine similarity. Similar observations for *lukewarm* and *bad*. But when we compare opposite polarity words like *love* and *bad*, we get negative cosine similarity values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9eab3-65ed-4210-b35a-9be2f787786a",
   "metadata": {},
   "source": [
    "## Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd0694-70e3-41be-8692-b4d159c5f77c",
   "metadata": {},
   "source": [
    "Next, we are going to build another feed-forward model, but this time, instead of using BOW features as input, we want to use the word sequence as input (so order of words is preserved). It is usually not straightforward to do this for classical machine learning models, but with neural networks and embeddings, it's pretty straightforward.\n",
    "\n",
    "Let's first build a pipeline by combining vocab and tokenizer together that can convert a sentence into a number sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0059c57-81cf-4856-825d-6fcf7e9dbe01",
   "metadata": {},
   "source": [
    "### Preparing sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b8ccc7e-4e6d-4458-9a9c-714f4c082b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1198, 476, 5, 357, 12, 9, 19, 222, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_pipeline = lambda x: vocab(tokenizer(x))\n",
    "sequence_pipeline(\"Hello world, today is a good day.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99407dc1-7f85-4255-b059-fc3f0060ac26",
   "metadata": {},
   "source": [
    "Now lets build the pytorch dataloader pipeline that 1.) Convert every text sentence into number sequence, 2.) Padding all sentences to our predefined max sentence, so the input dimension will be consistent, 3.) convert all input to appropriate tensor and place them on device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a766052-0e35-4b00-afe9-c6c926a6f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(sequence_pipeline(_text))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    \n",
    "    # Pad or truncate each sequence\n",
    "    padded_sequences = []\n",
    "    for seq in text_list:\n",
    "        # Truncate if longer than max_len\n",
    "        padded_seq = seq[:max_len]\n",
    "         # Pad if shorter\n",
    "        padded_seq += [padding_index] * (max_len - len(padded_seq)) \n",
    "        padded_sequences.append(torch.tensor(padded_seq))\n",
    "    text_list = torch.stack(padded_sequences)\n",
    "    # Stack all sequences into a single tensor\n",
    "    return text_list.to(device), label_list.reshape(-1, 1).to(device)\n",
    "\n",
    "# set max length\n",
    "max_len = 30\n",
    "\n",
    "# Create data loaders.\n",
    "xseq_train_dataloader = DataLoader(list(zip(sentences_train, y_train)), batch_size=10, collate_fn=seq_collate_batch)\n",
    "xseq_dev_dataloader = DataLoader(list(zip(sentences_dev, y_dev)), batch_size=10, collate_fn=seq_collate_batch)\n",
    "xseq_test_dataloader = DataLoader(list(zip(sentences_test, y_test)), batch_size=10, collate_fn=seq_collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032faab-1cd9-4f50-aac9-bd13b2bbafcf",
   "metadata": {},
   "source": [
    "## Sequence Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776c8e7-fde1-4f58-9a7b-dbf108384682",
   "metadata": {},
   "source": [
    "Now let's build our second model. This model first embeds each word in the input sequence into embeddings, and then concatenate the word embeddings together to represent input sequence. The ``Flatten`` function you see after the embedding layer is essentially doing the concatenation, by 'chaining' the list of word embeddings into a very long vector.\n",
    "\n",
    "If our word embeddings has a dimension 10, and our documents always have 30 words (padded), then here the concatenated word embeddings have a dimension of 10 x 30 = 300. \n",
    "\n",
    "The concatenated word embeddings undergo a linear transformation with non-linear activations (``layers.Dense(10, activation='relu')``), producing a hidden representation with a dimension of 10. It is then passed to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcae811f-7be3-407b-9cd6-4519ff479336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceFFNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding  = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        # torch.nn.init.uniform_(self.embedding.weight)\n",
    "        self.first_layer = nn.Linear(hidden_dim * max_len, hidden_dim)\n",
    "        # torch.nn.init.uniform_(self.first_layer.weight)\n",
    "        self.second_layer = nn.Linear(hidden_dim, 1)\n",
    "        # torch.nn.init.uniform_(self.second_layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # Flattening all word vectors in to one long vector\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.first_layer(x))\n",
    "        logits = torch.sigmoid(self.second_layer(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c6127-adc6-4c5e-adac-aefc030546ef",
   "metadata": {},
   "source": [
    "Now let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "51ec16ef-da8d-4f11-a3a9-ef567fa87c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "padding_index = vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d4675294-9f19-4a21-84c8-33e8fb724130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seqeunce feedforward network model!\n",
      "SequenceFFNetwork(\n",
      "  (embedding): Embedding(1814, 10, padding_idx=0)\n",
      "  (first_layer): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (second_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.680537 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.680779 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 0.682009 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.681219 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.677702 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.671548 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.668043 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.667923 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.676176 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.688505 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.700998 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.718900 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.744904 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.764508 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.790439 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.816102 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.840208 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.861304 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.886538 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.907068 \n",
      "\n",
      "Done!\n",
      "final test:\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.919655 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training seqeunce feedforward network model!\")\n",
    "\n",
    "seq_model = SequenceFFNetwork(vocab_size, embedding_dim, hidden_dim, padding_index, max_len=max_len).to(device)\n",
    "print(seq_model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(seq_model.parameters(), lr=0.001)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(xseq_train_dataloader, seq_model, loss_fn, optimizer)\n",
    "    test(xseq_dev_dataloader, seq_model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"final test:\")\n",
    "test(xseq_test_dataloader, seq_model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c30ed5-3fe5-4480-8058-7ba36cd358e5",
   "metadata": {},
   "source": [
    "You may find that the performance isn't as good as the BOW model. In general, concatenating word embeddings isn't a good way to represent word sequence.\n",
    "\n",
    "A better way is to build a recurrent model. But first, let's extract the word embeddings for the 4 words as before and look at their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3b565e17-fc17-491b-b4a2-fa04374c693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show sequence FF model embeddings\n",
      "show embedding similarity examples...\n",
      "embedding vector of love:\n",
      "[ 1.4589497   1.4694797  -0.43027857  1.4106635   0.2511162  -1.9204698\n",
      " -0.6061014   1.6407064  -0.35583866 -1.9419008 ]\n",
      "embedding cosine similarity comparisons:\n",
      "love vs. like = 0.014820153\n",
      "love vs. lukewarm = 0.24123268\n",
      "love vs. bad = 0.19129865\n",
      "lukewarm vs. bad = 0.509702\n"
     ]
    }
   ],
   "source": [
    "print(\"show sequence FF model embeddings\")\n",
    "# extract word embeddings layer\n",
    "embeddings = seq_model.embedding.weight.to(\"cpu\").detach().numpy()\n",
    "display_embedding_similarity_examples(embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc72bdd-098e-42b8-90e8-7f06e2e8835a",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334b104-47b2-40e0-bfc5-c7f1949b33ae",
   "metadata": {},
   "source": [
    "Now, let's try to build an LSTM model. After the embeddings layer, the LSTM layer will process the words one at a time, and compute the next state (dimension for the hidden state = 10 in this case). The output of the LSTM layer has three components \"output (the output values of all steps)\", \"hidden(the hidden state at the end of the LSTM)\", \"cell(the cell state at the end of the LSTM)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28e5bbc1-3eea-47ea-8061-089c8fc8a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding  = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.forward_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((1, batch_size, hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((1, batch_size, hidden_dim)).to(device)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq length]\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = [batch size, seq length, emb dim]\n",
    "\n",
    "        h0 = self.init_hidden(x.shape[0])\n",
    "\n",
    "        output, (hidden, cell) = self.lstm_layer(embedded, h0)\n",
    "        # output = [batch size, seq length, hid dim * num directions]\n",
    "        # hidden = [num layers * num directions, batch size, hid dim]\n",
    "        # cell = [num layers * num directions, batch size, hid dim]\n",
    "\n",
    "        hidden = hidden[-1, :, :]\n",
    "        # hidden = [batch size, hid dim]\n",
    "\n",
    "        logits = torch.sigmoid(self.forward_layer(hidden))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea78277-98a8-4bae-9eb1-fed2f89d6405",
   "metadata": {},
   "source": [
    "Let's see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "1f99d4db-e7cc-4ea7-8829-9ca493fc974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM network model!\n",
      "SimpleLSTMNetwork(\n",
      "  (embedding): Embedding(1814, 10, padding_idx=0)\n",
      "  (lstm_layer): LSTM(10, 10, batch_first=True)\n",
      "  (forward_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 0.697667 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.715024 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.717015 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.716857 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.716586 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.716263 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.715880 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 0.715420 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 0.714918 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.688075 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.671408 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 0.769836 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.667881 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.684113 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.655204 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.662305 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.680744 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.737220 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.723570 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.766725 \n",
      "\n",
      "Done!\n",
      "final test:\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.752546 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LSTM network model!\")\n",
    "\n",
    "lstm_model = SimpleLSTMNetwork(vocab_size, embedding_dim, padding_index).to(device)\n",
    "print(lstm_model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(xseq_train_dataloader, lstm_model, loss_fn, optimizer)\n",
    "    test(xseq_dev_dataloader, lstm_model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"final test:\")\n",
    "test(xseq_test_dataloader, lstm_model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc7d13-8f33-480c-adc3-c03098144bb4",
   "metadata": {},
   "source": [
    "You should notice that the training is quite a bit slower, and that's because now the model has to process the sequence one word at a time. But the results should be better than the sequence FFmodel!\n",
    "\n",
    "And lastly, let's extract the embeddings and look at the their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2e12a44b-4c24-4119-8c27-b2ae5a32b327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show LSTM model embeddings\n",
      "show embedding similarity examples...\n",
      "embedding vector of love:\n",
      "[-1.8690634  -0.8195206   0.3677807  -0.35293654 -1.1057066   0.4747699\n",
      " -0.50756794 -0.49564895  1.0310838   0.06060399]\n",
      "embedding cosine similarity comparisons:\n",
      "love vs. like = 0.58495873\n",
      "love vs. lukewarm = 0.14209871\n",
      "love vs. bad = 0.5900552\n",
      "lukewarm vs. bad = 0.5564137\n"
     ]
    }
   ],
   "source": [
    "print(\"show LSTM model embeddings\")\n",
    "# extract word embeddings layer\n",
    "embeddings = lstm_model.embedding.weight.to(\"cpu\").detach().numpy()\n",
    "display_embedding_similarity_examples(embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3511a00-9363-413e-b486-88542371b7e8",
   "metadata": {},
   "source": [
    "However, if you run the trainig a few times, you might notice that LSTM is not always better. In this particular case, the BOW approach seems to triumph over recurrent model. Why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683af49-bf61-40e7-8e8d-f118e4bbec78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
